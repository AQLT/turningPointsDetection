# Comparaison des différentes méthodes {#sec-comparison}

```{r, include = FALSE}
data_tp_ <- readRDS("data/data_tp.RDS")
data_tp <- lapply(data_tp_, function(x) lapply(x, table_tp))
data_tp_quar <- lapply(data_tp_, function(x) lapply(x, table_tp,quartile = TRUE))
caption_covid <- "%s du nombre de mois nécessaires pour détecter correctement un point de retournement de l'année 2020 (%i observations)."
caption_fc <- "%s du nombre de mois nécessaires pour détecter correctement un point de retournement entre 2007 et 2008 (%i observations)."
footnote <- c("Les statistiques ne sont calculés que pour les séries dont le filtre tendance-cycle optimal est de longueur 13.")
footnote2 <- "Pour 40 % des séries il faut moins de 3 mois pour détecter le bon point de retournement sans aucune révision future avec la méthode X-13ARIMA."

format.args = list(decimal.mark = ",",
                       nsmall = 1)

data_rev <- readRDS("data/revisions/tables_revisions_covid.RDS")
data_rev_quar <- readRDS("data/revisions/tables_revisions_covid_quartile.RDS")
data_rev_quar_kernel <- readRDS("data/revisions/tables_revisions_covid_kernels_quartile.RDS")

caption_rev <- "%s de la racine carrée de l'erreur quadratique moyenne entre la première et la dernière estimation sur l'année 2020 (%i observations)."

```

## Méthodologie

Dans cette partie, on compare les différentes moyennes mobiles en termes de délai pour détecter les points de retournement.
Dans ce rapport, on définit de la façon suivante les points de retournement, en distinguant deux cas :

- les redressements (*upturn*) où l'on passe d'une phase de récession à une phase d'expansion de l'économie (comme c'est généralement le cas après une crise économique). 
C'est le cas à la date $t$ lorsque $y_{t-k}\geq\cdots\geq y_{t-1}<y_t\leq y_{t+1}\leq\cdots y_{t+m}$.

- les ralentissements (*downturn*) où l'on passe d'une phase d'expansion à une phase de récession.
C'est le cas à la date $t$ lorsque $y_{t-k}\leq\cdots\leq y_{t-1}>y_t\geq y_{t+1}\geq\cdots y_{t+m}$.

En suivant @Zellner1991, du fait du caractère relativement lisse de la tendance-cycle, on retient les paramètres $k=3$ and $m=1$. 

Les différents filtres sont comparés sur la base d'Eurostat des indices de production industriels déjà corrigés des jours ouvrables (`sts_inpr_m`, 2 404 séries connues jusqu'en mars 2021).
Les points de retournement finaux sont ceux qui sont trouvés sur la composante tendance-cycle estimée avec la méthode X-13ARIMA sur l'ensemble de la période.
Le déphasage est défini comme le nombre de mois nécessaires pour détecter le bon point de retournement sans aucune révision future (il est donc possible que ce déphasage ne soit pas calculable si pour une méthode donnée on ne détecte pas le bon point de retournement à la dernière date d'estimation).

Pour chaque série et chaque date, la méthodologie suivante est utilisée :

1. La série est désaisonnalisée avec la méthode X-13ARIMA^[
On utilise pour cela la fonction `x13()` du package `RJDemetra` avec le paramètre `spec = "RSA3"` (détection automatique des différents paramètres sans correction des jours ouvrables).
] pour extraire différents paramètres qui seront fixés entre les différentes méthodologies : la série linéarisée (i.e.; la série initiale corrigée de points atypiques par un Reg-ARIMA), la longueur des filtres saisonniers et du filtre utilisé pour extraire la tendance-cycle, le schéma de décomposition et l'I-C ratio.

2. Une nouvelle désaisonnalisation est effectuée sur la série linéarisée en utilisant l'algorithme de décomposition de X-13ARIMA et en utilisant les mêmes paramètres que précédemment et en changeant uniquement les filtres asymétriques utilisés pour extraire la tendance-cycle^[
Cela peut être fait en utilisant la fonction `rjdfilters::x11()`.
].  
Pour contrôler l'impact des estimations des paramètres et des points atypiques dans le modèle Reg-ARIMA, la série linéarisée utilisée ici correspond à celle calculée sur l'ensemble de la série.  
Pour la méthode de référence (X-13ARIMA), la série linéarisée est prolongée par prévision via un modèle ARIMA sur un an avant la décomposition mais pour toutes les autres méthodes on n'utilise pas les prévisions dans la décomposition.  
Une description plus détaillée de l'algorithme de décomposition est disponible en annexe \@ref(an-x11).

3. Pour chaque estimation de la tendance-cycle, on calcule les points de redressement et de ralentissement.

Dans les différents résultats, on se restreint aux séries pour lesquelles le filtre symétrique optimal utilisé pour extraire la tendance-cycle est de longueur 13 (valeur majoritairement retenue).
Il n'y a pas d'impact du schéma de décomposition sur les résultats.


## Comparaison des filtres polynomiaux locaux et des filtres RKHS

Du fait du fort degré de liberté de l'approche FST (dans le choix des différents paramètres), dans cette section, on ne compare la méthode X-13ARIMA unique à l'approche polynomiale locale et aux filtres RKHS.
Pour les filtres RKHS on ne retient que ceux qui minimisent la *timeliness* puisque dans notre cas on souhaite minimiser le délai dans la détection de points de retournement.

Le tableau \@ref(tab:covid-quantile) compare les différentes méthodes pour la détection des points de retournement durant la crise du COVID-19 (i.e., durant l'année 2020).
La méthode LC (i.e., les filtres de Musgrave) semble être, en distribution, plus performante que les autres méthodes (avec des performances similaires à X-13ARIMA) et la méthode la moins performante semble être les filtres RKHS obtenus en minimisant la *timeliness*. 
La moins bonne performance des filtres QL, CQ et DAF par rapport au filtre LC pourrait être expliquée par la plus grande volatilité des estimations du fait des contraintes plus fortes sur la préservation des tendances polynomiales.

```{r covid-quantile, fig.note = c(footnote)}
data <- data_tp$covid$full$table[,1:6]
colnames(data)[2] <- "RKHS ($b_{q,\\varphi}$)"
kbl(data,
    caption = sprintf(caption_covid, "Déciles", data_tp$covid$full$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_footnote_kable()
```

Deux exemples sont présentés dans l'annexe \@ref(an-plotseries) : un où la méthode X-13ARIMA détecte plus rapidement le point de retournement que les autres et un ou c'est l'inverse.
On retrouve le fait que les estimations avec les méthodes QL, CQ et DAF sont fortement bruitées avec des estimations qui ne sont pas plausibles (notamment celles autour de points de retournement).

Le tableau \@ref(tab:covid-rev) montre la distribution de la racine carrée de l'erreur quadratique moyenne de révision (RMSE) entre la première et la dernière estimation de la tendance-cycle.
La meilleure méthode est cette fois-ci la méthode actuelle (X-13ARIMA), suivie du filtre RKHS et ensuite la même hiérarchie que précédemment se retrouve entre les différentes méthodes polynomiales locales.
Toutefois, comme souligné dans la partie \@ref(subec:mmetprev), la méthode X-13ARIMA revient à utiliser un filtre asymétrique dont les poids sont optimisés pour minimiser la révision entre la première et la dernière estimation.
Par construction, l'indicateur de révision retenu privilégie donc la méthode X-13ARIMA : il faudrait utiliser un indicateur qui prennent également en compte les révisions entre les autres horizons de prévision (par exemple entre la première et la deuxième estimation, deuxième et dernière estimation, etc.).

```{r covid-rev, fig.note = c(footnote)}
data_table_rev <- data_rev$RMSE[,1:6]
colnames(data_table_rev) <- colnames(data)
rownames(data_table_rev) <- rownames(data)
kbl(data_table_rev,
    caption = sprintf(caption_rev, "Déciles", data_tp$covid$full$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_footnote_kable()
```


Le même exercice peut être fait en analysant le déphasage dans la détection des points de retournement pendant la crise financière de 2008.
En revanche, contrairement à la crise du COVID-19, toutes les séries n'ont pas été impactées au même mois : une analyse plus approfondie devrait être faite pour vérifier si les points détectés sont réellement des points de retournement. 
Même les résultats du tableau \@ref(tab:covid-quantile-crisis) doivent être étudiés avec précaution, on observe la même hiérarchie entre les méthodes que celle trouvée pendant l'étude de la crise du COVID-19. 
Par ailleurs, pour plus de 80 % des séries et pour toutes les méthodes, le déphasage dans la détection des points de retournement est supérieur à 6 mois.
C'est-à-dire que dans la majorité des cas, la détection des points de retournement n'est pas stable, même si l'on utilise un filtre symétrique pour l'estimation de la tendance-cycle.
Cela vient notamment de l'algorithme de désaisonnalisation qui, pour estimer les coefficients saisonniers utilisés pour corriger la série où l'on applique les filtres pour extraire la tendance-cycle, a besoin de bien plus que 6 mois dans le futur.
Par exemple, en utilisant les paramètres les plus souvent utilisés dans X-13ARIMA, le filtre final symétrique utilisé pour extraire la tendance-cycle a 169 termes !

```{r covid-quantile-crisis, fig.note = c(footnote)}
data <- data_tp$financial_crisis$full$table[,1:6]
colnames(data)[2] <- "RKHS ($b_{q,\\varphi}$)"
kbl(data,
    caption = sprintf(caption_fc, "Déciles", data_tp$financial_crisis$full$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_footnote_kable()
```


## Comparaison avec l'approche FST

Pour limiter les degrés de liberté dans le choix des poids dans l'approche FST, une approche peut être de les sélectionner comparativement à une autre moyenne mobile.
En effet, si l'on se concentre sur les trois critères de *fidelity*, *smoothness* et *timeliness*, comparativement à une moyenne mobile fixée, un ensemble de poids peut être considéré comme optimal s'il conduit à un filtre de meilleure qualité (selon ces trois critères) avec les mêmes contraintes polynomiales (ou des contraintes polynomiales plus grandes).
L'annexe \@ref(sec-annexeFST) trace l'ensemble de ces poids pour les filtres locaux polynomiaux et les filtres RKHS mais concentrons-nous ici sur le cas $h=6$ (filtre symétrique de 13 termes).

Parmi l'ensemble des poids "admissibles" (i.e.; un ensemble de poids minimisant les 3 critères par rapport à un filtre de référence), plusieurs méthodes peuvent être utilisées pour construire les moyennes mobiles asymétriques.
Ici nous en retenons trois :

1. on retient la moyenne mobile qui minimise la *timeliness* pour limiter au mieux le déphasage (i.e.; celle dont le poids associé à ce critère est le plus fort) ;

2. pour éviter d'introduire trop de variance dans les estimations, on retient la moyenne mobile dont le poids associé à la *timeliness* est la plus faible ;

3. on adopte une position médiane en retenant la moyenne mobile dont le poids associé à la *timeliness* est médian parmi l'ensemble des poids admissibles.

Parmi les approches polynomiales, le filtre LC est celui qui semble donner les meilleurs résultats : nous nous restreignons donc à la comparaison avec cette méthode. 
Pour le choix du degré de préservation, pour la première méthode, forcer, lorsque c'est possible, la moyenne mobile à préserver les polynômes de degré 1 ne change pas les poids retenus : c'est ce qui a été fait pour cette méthode. Pour les deux autres, nous nous sommes restreints aux moyennes mobiles préservant les constantes uniquement (comme le filtre LC).
Par simplicité, le filtre FST optimal associé au filtre RKHS n'a été étudié qu'avec la première méthode.
Ces poids sont présentés dans le tableau \@ref(tab:poidsfstsimul), et les coefficients associés sont présentés dans l'annexe \@ref(an-graphs) dans les figures \@ref(fig:graphsfstlcmax) à \@ref(fig:graphsfstrkhs).
Le filtre utilisé pour l'estimation en temps réel ($q=0$) qui minimise la timeliness par rapport au filtre LC accorde, pour l'estimation de la tendance-cycle en $t$, un poids plus important à la valeur observée en $t-1$ qu'à celle observée en $t$ (figure \@ref(fig:graphsfstlcmax)) : cela ne parait pas plausible économiquement et justifie de ne pas se restreindre uniquement à la première méthode de sélection des poids.


```{r poidsfstsimul}
tab <- lapply(readRDS("data/comparaison_fst/fst_h6.RDS"), `[[`, "final_weights")
tab <- tab[c("lc", "lc_min", "lc_med", "rkhs_timeliness")]
tab_format <- do.call(rbind, lapply(tab, function(x){
    t(x[,c("smoothness.weight", "timeliness.weight", "fidelity.weight")])
}))
colnames(tab_format) <- sprintf("$q=%i$", 0:5)
rownames(tab_format) <- gsub(".weight", "", rownames(tab_format), fixed = TRUE)
rownames(tab_format) <- capitalize(rownames(tab_format))

format.args2 = list(decimal.mark = ",",
                   nsmall = 3)
index_row  <- c("LC min. timeliness" = 3,
                      "LC max. timeliness" = 3,
                      "LC timeliness médiane" = 3,
                      "RKHS min. timeliness" = 3)
kbl(tab_format, caption = "Poids retenus pour construire des filtres avec l'approche FST",
    booktabs = TRUE,
    format.args = format.args2, escape = FALSE,
    align = "c", digits = 3)%>%
    kable_styling(latex_options = c("striped", "hold_position"))%>%
    pack_rows(index=index_row)
```

En termes de détection des points de retournement (tableau \@ref(tab:covid-tp-fst)), le déphasage semble plus grand pour ces filtres que pour les autres méthodes. 
Sur les deux exemples étudiés (annexe \@ref(an-plotseries)), les estimations sont bien moins volatiles que pour les méthodes polynomiales QL, CQ et DAF^[
Cela pouvait d'ailleurs s'anticiper par l'analyse des fonctions de gains présentés en annexe \@ref(an-graphs). 
En effet, pour les autres méthodes, pour les filtres asymétriques utilisés lors de l'estimation en temps réel ($q=0$), la fonction de gain est supérieure à 1 pour de nombreuses fréquences, ce qui signifie que le bruit est amplifié.
Toutefois, pour les basses fréquences, les fonctions de gains décroissent plus vite pour les filtres issus de la méthode FST, ce qui signifie que la composante tendance-cycle est atténuée.
]. 
En revanche, les estimations intermédiaires ($q=4$ ou $q=5$) semblent assez erratiques. 
Cela pourrait provenir du fait que les coefficients associés à ces moyennes mobiles donnent un poids relativement faible à l'estimation courante (les coefficients associés aux observations en $t-1$, $t$ et $t+1$ sont très proches).

```{r covid-tp-fst, fig.note = c(footnote)}
data <- data_tp_quar$covid$full$table[,-c(2:6)]
colnames(data)[-1] <- c("Min.", "Max.", 
                        "Méd.", "Min.")
# colnames(data) <- gsub("(LC)|(RKHS) ","", colnames(data))
#colnames(data)[2] <- "RKHS ($b_{q,\\varphi}$)"
kbl(data,
    caption = sprintf(caption_covid,  "Quartiles",data_tp$covid$full$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_header_above(c(" "=2, "FST - LC" = 3, "FTS - RKHS" = 1)) %>% 
    add_footnote_kable()
```

En revanche, en termes de révisions entre la première et la dernière estimation (tableau \@ref(tab:covid-rev-fst)), ces filtres asymétriques sont plus performants que les méthodes polynomiales locales et les filtres RKHS.

```{r covid-rev-fst, fig.note = c(footnote)}
data_table_rev <- data_rev_quar$RMSE[,-c(2:6)]
colnames(data_table_rev) <- colnames(data)
rownames(data_table_rev) <- rownames(data)
kbl(data_table_rev,
    caption = sprintf(caption_rev,"Quartiles", data_tp$covid$full$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_header_above(c(" "=2,"FST - LC" = 3, "FTS - RKHS" = 1)) %>% 
    add_footnote_kable()
```

Notons par ailleurs que la hiérarchie entre les méthodes sur les révisions entre la première et la dernière observation, estimée empiriquement dans les tableaux \@ref(tab:covid-rev) et \@ref(tab:covid-rev-fst), se retrouve dans les indicateurs théoriques, définis dans la section \@ref(sec-WildiMcLeroy). C'est ce qui est montré par le tableau \@ref(tab:tab-mse-q0), qui compare les filtres asymétriques utilisés pour l'estimation en temps réel ($q=0$), en supposant que les séries sont des bruits blancs.

```{r, inclue = FALSE}
mse_theo <- readRDS("data/mse_theo.RDS")
title <- "Décomposition de l'erreur quadratique moyenne des filtres asymétriques utilisés en temps réel ($q=0$)."
footnote_mse <- "Indicateurs calculés en supposant que la série initiale est un bruit blanc ($h(\\omega)=1$)."
```

```{r tab-mse-q0, fig.note = footnote_mse}
kbl(mse_theo,
    caption = title,
    booktabs = TRUE,
    format.args = list(decimal.mark = ",",
                   nsmall = 1),
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_header_above(c(" "=6, "FST - LC" = 3, "FTS - RKHS" = 1))%>% 
    add_footnote_kable()
```

Plusieurs conclusions peuvent être tirées de ces résultats :

- Minimiser la *timeliness* a surtout du sens lorsque peu de données dans le futur sont connues : dès lors que l'on s'approche du cas symétrique, cela ne parait plus pertinent et peut conduire à des résultats indésirables (estimations erratiques).

- Il n'est pas forcément pertinent d'utiliser la même méthode de construction pour les filtres utilisés en temps réel ($q=0$) et pour ceux utilisés lorsque l'on s'approche du cas symétrique.

- La méthode utilisée pour mesurer le déphasage privilégie les filtres qui minimisent la révision au filtre symétrique (filtres polynomiaux locaux). 
En effet, pour les autres (RKHS et filtres FST) il faut, dans la majorité des cas, au moins 6 mois pour détecter correctement le point de retournement : cela signifie qu'il y a eu une révision du point de retournement au moment de l'utilisation du filtre symétrique. 
Cela rejoint les deux points précédents : les moyennes mobiles asymétriques utilisées lorsque l'on s'approche du cas symétrique sont sûrement sous-optimales.

Une piste d'amélioration pourrait donc être d'étudier les filtres LC calculés en rajoutant la *timeliness* dans le programme de minimisation, comme présenté dans la section \@ref(subsec-lptimeliness).
Plus d'études devront être menées pour savoir comment calibrer le poids associé à cet indicateur.

## Impact du noyau

Dans cette section, nous vérifions l'a priori que le choix du noyau n'est pas fondamental et que dans les approches polynomiales et RKHS, on peut se restreindre à l'étude d'un seul noyau.
Pour cela, nous comparons, en utilisant uniquement la méthode *Linear-Constant* (LC), la performance des filtres asymétriques obtenus en utilisant les différents noyaux présentés dans la section \@ref(sec-kernels).
En termes de déphasage pour la détection de points de retournement (tableau \@ref(tab:covid-tp-kernels)), tous les noyaux ont des performances similaires, sauf pour le noyau uniforme (qui est sous-optimal car accorde un poids équivalent à toutes les observations).
Les résultats sont également très proches en termes de révision (tableau \@ref(tab:covid-rev-kernels)) : le choix du noyau ne semble donc pas fondamental.


```{r covid-tp-kernels, fig.note = c(footnote)}
data <- data_tp_quar$covid$kernels$table
#colnames(data)[2] <- "RKHS ($b_{q,\\varphi}$)"
colnames(data) <- capitalize(colnames(data))
kbl(data,
    caption = sprintf(caption_covid,"Quartiles", data_tp$covid$kernels$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_footnote_kable()
```

```{r covid-rev-kernels, fig.note = c(footnote)}
data_table_rev <- data_rev_quar_kernel$RMSE
colnames(data_table_rev) <- colnames(data)
rownames(data_table_rev) <- rownames(data)

kbl(data_table_rev,
    caption = sprintf(caption_rev,"Quartiles", data_tp$covid$kernels$n),
    booktabs = TRUE,
    format.args = format.args,
    escape = FALSE,
    align = "c", digits = 1)  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    add_footnote_kable()
```


## Discussion

Comme nous l'avons vu dans les sections précédentes, la méthodologie et les indicateurs utilisés pour comparer les différentes méthodes sont perfectibles. 
Toutefois, les différents résultats montrent que, pour la construction des moyennes mobiles asymétriques, on peut se restreindre à celles qui conserve les constantes : chercher à imposer des contraintes polynomiales plus élevées augmente la variance des estimations et le délai pour détecter des points de retournement.
Ainsi, dans les méthodes polynomiales locales, on peut restreindre les prochaines études à la méthode *Linear-Constant* (LC).
Par ailleurs, l'estimation en temps réel des méthodes utilisant les filtres asymétriques direct (DAF) peut être facilement améliorée, comme c'est par exemple le cas de la méthode de désaisonnalisation STL (*Seasonal-Trend decomposition based on Loess*) proposée par @cleveland90.

Sur les méthodes ici étudiées, il serait intéressant de continuer l'exploration pour essayer de comprendre quand une méthode est plus performante qu'une autre, ce qui permettrait de donner des recommandations sur la méthode à utiliser.
Il faudrait également veiller à comparer les méthodes sur d'autres types de bases de données pour vérifier si les résultats ne dépendent pas du type de série en entrée.

Toutes les méthodes de construction des filtres asymétriques sont calibrées en utilisant des paramètres estimés sur l'ensemble de la période.
Par exemple, on utilise autant de points dans le passé que le filtre symétrique alors que rien ne suggère que c'est la longueur optimale. 
Ainsi, le ratio $I/C$ utilisé pour choisir la longueur du filtre de la tendance-cycle et pour calibrer les méthodes locales polynomiales, est calculé sur l'ensemble de la série en excluant les 6 premiers et 6 derniers mois, qui sont justement ceux étudiés !
Pour la construction des filtres utilisés pour l'estimation en temps réel, il pourrait donc être préférable d'utiliser d'autres paramètres, par exemple des estimations locales de $I/C$ (c'était d'ailleurs une des pistes de recherche suggérées par @GrayThomson1996).
De manière alternative, @vasyechko2014new proposent de toujours utiliser un filtre de 13 termes : pour l'estimation en temps réel ($q=0$) le filtre utilise l'observation courante et les 12 observations précédentes, lorsqu'on connait un point dans le futur ($q=1$) on utilise les 11 observations précédant la date courante, etc.

Les indicateurs utilisés dans cette partie semblent avantager, par construction, la méthode actuelle X-13ARIMA. 
Cela pourrait également être le cas de la méthode d'estimation utilisée.
En effet, dans les simulations, nous utilisons une série corrigée des points atypiques par un modèle Reg-ARIMA estimée sur l'ensemble de la période.
Pour les simulations avec la méthode X-13ARIMA, pour chaque date, la série est ensuite prolongée sur un an par un modèle ARIMA. 
Toutefois, de cette façon les prévisions sont toujours réalisées dans des cas "simples" puisqu'on ne prend pas en compte l'impact, sur la prévision, de la détection et de l'estimation en temps réel des points atypiques.
Il serait donc intéressant de comparer les différentes méthodes en réestimant à chaque date la série linéarisée et donc en se plaçant réellement en estimation en temps réel.

Enfin, dans les articles associés aux méthodes étudiées dans ce rapport, les filtres asymétriques sont appliqués et comparés sur des séries déjà désaisonnalisées. 
Le déphasage dans la détection des points de retournement est donc d'au plus 5 mois (lorsque le filtre symétrique est de 13 termes). 
Cette méthode, beaucoup plus simple que celle utilisée dans ce rapport, peut néanmoins conduire à deux écueils :

1. D'une part, l'estimation de la série désaisonnalisée dépend de la méthode utilisée pour extraire la tendance-cycle et comme nous l'avons vu dans l'analyse des points de retournement entre 2007 et 2008, le choix de la méthode utilisée pour l'estimation de la tendance-cycle a un impact bien au-delà de 13 mois.
Les exemples devraient toutefois être approfondis pour comprendre si les points en question correspondent à de vrais points de retournement.

2. D'autre part, les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire donc à des estimations biaisées du fait la présence de points atypiques, alors que la méthode X-13ARIMA a un module de correction des points atypiques.
Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsque l'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois.
Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois.
Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement.
Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier). 
C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à :

    a. appliquer l'algorithme de correction des points atypique de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ;
    
    b. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict ($\pm0,7\sigma$ et $\pm 1,0 \sigma$, voir annexe \@ref(an-x11)) et appliquer ensuite le filtre symétrique de 13 termes. 
    En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.   
    
Une piste d'étude serait alors d'étudier plus précisément l'impact des points sur l'estimation de la tendance-cycle et la détection des points de retournement, mais aussi d'explorer de nouveaux types de filtres asymétriques basés sur des méthodes robustes (comme les régressions locales robustes, les médianes mobiles, etc.).
 





\newpage
