# Comparaison des différentes méthodes {#sec-comparison}

```{r, include = FALSE}
data_tp_ <- readRDS("data/data_tp.RDS")
data_tp <- lapply(data_tp_, function(x) lapply(x, table_tp))
data_tp_quar <- lapply(data_tp_, function(x) lapply(x, table_tp,quartile = TRUE))
caption_covid <- "%s du nombre de mois nécessaires pour détecter correctement un point de retournement de l'année 2020 (%i observations)."
caption_fc <- "%s du nombre de mois nécessaires pour détecter correctement un point de retournement entre 2007 et 2008 (%i observations)."
footnote <- c("Les statistiques ne sont calculés que pour les séries dont le filtre tendance-cycle optimal est de longueur 13.")
footnote2 <- "Pour 40 % des séries il faut moins de 3 mois pour détecter le bon point de retournement sans aucune révision future avec la méthode X-13ARIMA."

format.args = list(decimal.mark = ",",
                       nsmall = 1)

data_rev <- readRDS("data/revisions/tables_revisions_covid.RDS")
data_rev_quar <- readRDS("data/revisions/tables_revisions_covid_quartile.RDS")
data_rev_quar_kernel <- readRDS("data/revisions/tables_revisions_covid_kernels_quartile.RDS")

caption_rev <- "%s de la racine carrée de l'erreur quadratique moyenne entre la première et la dernière estimation sur l'année 2020 (%i observations)."

```

# Méthodologie

Les différentes méthode de construction de moyennes mobiles asymétriques sont comparées sur des données simulées et des données réelles. Pour toutes les séries, un filtre symétrique de 13 termes est utilisé.

En suivant une méthodologie proche de celle de @DarneDagum2009, neuf séries mensuelles sont simulées entre janvier 1960 et décembre 2020 avec différent niveaux de variabilité. Chaque série simulée $y_t= C_t+ T_t + I_t$ peut s'écrire comme la somme de trois composantes :

- le cycle $C_t = \rho [\cos (2 \pi t / \lambda) +\sin (2 \pi t / \lambda)]$, $\lambda$ est fixé à 72 (cycles de 6 ans, il y a donc 19 points de retournement détectables) ;

- la tendance $T_t = T_{t-1} + \nu_t$ avec $\nu_t \sim \mathcal{N}(0, \sigma_\nu^2)$, $\sigma_\nu$ étant fixé ) $0,08$ ;

- et l'irrégulier $I_t = e_t$ avec $e_t \sim \mathcal{N}(0, \sigma_e^2)$.

Pour les différentes simulations, nous faisons varier les paramètre $\rho$ et $\sigma_e^2$ afin d'avoir d'avoir des séries avec différents rapports signal sur bruit :

- Fort rapport signal sur bruit (c'est-à-dire un I-C ratio faible et une faible variabilité) : $\sigma_e^2=0,2$ et $\rho = 3,0,\, 3,5$ ou $4,0$ (I-C ratio compris entre 0,9 et 0,7) ;

- Rapport signal sur bruit moyen (c'est-à-dire un I-C ratio moyen et une variabilité moyenne) : $\sigma_e^2=0,3$ et $\rho = 1,5,\, 2,0$ ou $3,0$ (I-C ratio compris entre 2,3 et 1,4) ;

- Faible rapport signal sur bruit (c'est-à-dire un I-C ratio fort et une forte variabilité) : $\sigma_e^2=0,4$ et $\rho = 0,5,\, 0,7$ ou $1,0$ (I-C ratio compris entre 8,9 et 5,2).

Pour chaque série et chaque date, la tendance-cycle est estimée en utilisant les différentes méthodes présentées dans de ce rapport.
Pour les régressions polynomiales locales, les filtres asymétriques sont calibrés en utilisant l'I-C ratio estimé à chaque date (en appliquant un filtre de Henderson de 13 termes) et pour la méthode FST, un quadrillage du plan est réalisé avec un pas de $0,05$ et avec comme contraintes linéaires la préservation des polynômes de degrés 0 à 3.
Trois critères de qualité sont également calculés :

1. Calcul du déphasage dans la détection des points de retournement. La définition @Zellner1991 est utilisée pour déterminer les points de retournement, en distinguant deux cas :  
    - les redressements (*upturn*) où l'on passe d'une phase de récession à une phase d'expansion de l'économie. 
    C'est le cas à la date $t$ lorsque $y_{t-3}\geq y_{t-2}\geq y_{t-1}<y_t\leq y_{t+1}$.  
    - les ralentissements (*downturn*) où l'on passe d'une phase d'expansion à une phase de récession.
C'est le cas à la date $t$ lorsque $y_{t-3}\leq y_{t-2}\leq y_{t-1}>y_t\geq y_{t+1}$.  

    Le déphasage est souvent définit comme le nombre de mois nécessaires pour détecter le bon point de retournement (i.e. : le point de retournement sur la composante cyclique).
    Nous utilisons ici un critère légèrement modifié : le déphasage est définit comme le nombre de mois nécessaires pour détecter le bon point de retournement sans aucune révision future.
    Il peut en effet arriver que le bon point de retournement soit détecté par des filtres asymétriques mais ne le soit pas avec l'estimation finale avec un filtre symétrique (c'est le cas de 41 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave) ou qu'il y ait des révisions dans les estimations successives (c'est le cas de 7 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave).
    Finalement, relativement peu de points de retournement sont détectés à la bonne date avec l'estimation finale.
    Avec le filtre de Henderson de 13 termes, 18 sont correctement détectés sur les séries avec une faible variabilité (sur les 57 possibles), 11 sur les séries à variabilité moyenne et 12 sur les séries à forte variabilité.

2. Calcul de deux critères de révisions : la moyenne des écarts relatifs entre la $q$\ieme{} estimation et la dernière estimation $MAE_{fe}(q)$ et la moyenne des écarts relatifs entre la $q$\ieme{} et la $q+1$\ieme{} estimation $MAE_{qe}(q)$
$$
MAE_{fe}(q)=\mathbb E\left[
\left|\frac{
y_{t|t+q} -  y_{t|last}
}{
 y_{t|last}
}\right|
\right]
\quad\text{et}\quad
MAE_{qe}(q)=\mathbb E\left[
\left|\frac{
y_{t|t+q} - y_{t|t+q+1}
}{
y_{t|t+q+1}
}\right|
\right]
$$

## Comparaison des filtres polynomiaux locaux et des filtres RKHS

Du fait du fort degré de liberté de l'approche FST (dans le choix des différents paramètres), dans cette section, on ne compare les méthodes issues l'approche polynomiale locale utilisant le noyau de Henderson^[
Il est en effet difficile de comparer proprement les résultats entre les différents noyaux car le filtre symétrique n'est pas le même. Cela a pour conséquence que des points de retournement peuvent être détectés. Par exemple, pour le filtre LC, sur les trois séries ayant une variabilité moyenne, seul 1 point de retournement est correctement détecté par l'ensemble des noyaux.
Toutefois, une première analyse des résultats montrent que les différents noyaux ont des performances proches en termes de déphasage et de révisions, sauf le noyau uniforme qui produit de moins bons résultats.
] et les filtres RKHS $b_{q,\Gamma}$, $b_{q,G}$ et $b_{q,\varphi}$.

En termes de déphasage dans la détection des points de retournement, c'est le filtre $b_{q,G}$ qui semble donner les meilleurs résultats quelle que soit la variabilité de la série (graphique \@(fig:graphstpsimul)).
Étonnement, c'est le filtre $b_{q,\varphi}$ qui minimise le déphasage qui donne les moins bons résultats. Cela peut s'expliquer par le fait que la courbe des coefficients des moyennes mobiles asymétriques sont assez éloignées des coefficients du filtre symétrique : il y a donc potentiellement beaucoup de révisions dans la détection des points de retournement.
Dans les filtres polynomiaux locaux, les filtres LC (filtres de Musgrave) semblent donner les meilleurs résultats lorsque la variabilité est moyenne ou forte (avec des résultats proches du filtre QL dans ce second pas) mais de moins bons résultats lorsque la variabilité est faible.

Concernant les révisions, la variabilité de la série a peu d'impact sur les performances respectives des différentes méthodes mais joue sur les ordres de grandeurs.
Globalement, les filtres LC minimisent toujours les révisions (voir tableau \@ref(tab:simulrev)) et les révisions sont plus importantes avec les filtres CQ, DAF et les filtres RKHS autres que $b_{q,\varphi}$. 
Pour les filtres QL, il y a une forte révision entre la deuxième et la troisième estimation : cela peut venir du fait que pour la deuxième estimation (lorsque l'on connait un point dans le futur), le filtre QL associe un point plus important à l'estimation en $t+1$ qu'à l'estimation en $t$, ce qui crée une discontinuité.
Pour les filtres polynomiaux autres que le filtre LC, les révisions importantes à la première estimation étaient prévisibles au vu de la courbe des coefficients : un poids très important est associé à l'observation courante et il y une forte discontinuité entre la moyenne mobile utilisée pour l'estimation en temps réelle (lorsqu'aucun point dans le futur n'est connu) et les autres moyennes mobiles.

En somme, par rapport au filtre LC, la réduction du déphasage du filtre $b_{q,G}$ se fait au coût de révisions 4 fois plus importantes lorsque la variabilité de la série est moyenne. Pour les séries à forte variabilité, les révisions sont du même ordre de grandeur mais l'écart est bien plus important pour les séries à faible variabilité.

```{r graphstpsimul, echo=FALSE, out.width="100%", fig.cap="Distribution des déphasages sur les séries simulées."}
img <- sprintf("img/simulations/phase_shift_simul.%s", fig.ext)
knitr::include_graphics(img)
```


```{r simulrev, echo = FALSE}
title = "Moyenne des écarts relatifs des révisions pour les différentes filtres sur les séries à variabilité moyenne."
rev_table <- readRDS( "data/simulations_revisions.RDS")
rev_table  %>%
  kable(format.args = list(digits = 2,
                           decimal.mark = ","),
        align = "c", booktabs = T, row.names = FALSE, 
        escape = FALSE, caption = title) %>%  
  kable_styling(latex_options=c(#"striped",  
    "hold_position")) %>% 
  pack_rows(index = c("$MAE_{fe}(q) = \\mathbb E\\\\left[\\\\left|(y_{t|t+q} -  y_{t|last})/y_{t|last}\\\\right|\\\\right]$"=7,
                      "$MAE_{ce}(q)=\\mathbb E\\\\left[
\\\\left|(y_{t|t+q} - y_{t|t+q+1})/y_{t|t+q+1}\\\\right|
\\\\right]$"=7), escape = FALSE)
```


## Comparaison avec l'approche FST

Pour le choix des poids dans l'approche FST, l'idée retenue dans cette étude est de faire un quadrillage du plan $[0,1]^3$ avec un pas de 0,05 et en imposant $\alpha + \beta + \gamma = 1$^[
Comme il n'est pas possible d'avoir un poids associé à la *timeliness* ($\gamma$) égale à 1 (sinon la fonction objectif n'est pas strictement convexe), on construit également un filtre avec un poids très proche de 1 ($1-1/1000$).
]. 
Pour chaque combinaison de poids, quatre ensembles de moyennes mobiles sont construits en forçant dans la minimisation la préservation de polynômes de degré 0 à 3. 
Le filtre symétrique utilisé est toujours celui de Henderson. 
Ces différentes moyennes mobiles sont ensuite comparées relativement aux performances des filtres LC et, par simplification, uniquement sur les séries simulées à variabilité moyenne.

En termes de déphasage, en médiane, les filtres qui sont plus performants que les filtres LC sont ceux qui préservent le polynômes de degré 2 et ayant un poids associé à la *fidelity* ($\beta$) inférieur à 0,5 et ceux qui préservent les polynômes de degré 3 (graphique \@ref(fig:graphstpsimulfst)). 
En revanche, une analyse plus fine des résultats montre qu'en moyenne le déphasage est plus élevé qu'avec la méthode LC pour tous les filtres FST mais les résultats sont quasiment équivalents (entre 1,0 et 1,1) pour les filtres qui préservent les polynômes de degré 2 avec $\alpha = \beta =0,05$ et $\alpha = 0,05, \, \beta =0$ et ceux qui préservent les polynômes de degré 3 avec $\beta=0$.


```{r graphstpsimulfst, echo=FALSE, out.width="90%", fig.cap="Médiane du déphasage relatif des filtres FST par rapport au filtre LC selon les poids sur les séries simulées à variabilité moyenne."}
img <- sprintf("img/simulations/fst_tp_med_mediumvariability.%s", fig.ext)
knitr::include_graphics(img)
```

En termes de révisions (graphiques \@ref(fig:graphsfeq0simulfst) et \@ref(fig:graphsceq0simulfst)), les révisions entre la première et dernière estimation et entre la première et deuxième estimation sont inférieures à celles du filtre LC lorsque les filtres préservent les polynômes de degré 1. En revanche, avec les filtres qui préserve les polynômes de degré 3 les révisions entre la première et dernière estimation sont en moyenne plus de deux fois plus importante qu'avec le filtre LC et sont modérément plus élevées (rapport entre 1 et 2).

```{r graphsfeq0simulfst, echo=FALSE, out.width="90%", fig.cap="Moyenne des écarts relatifs des révisions entre la première et la dernière estimation ($MAE_{fe}(0)$), comparativement aux révisions du filtre LC sur les séries à variabilité moyenne."}
img <- sprintf("img/simulations/fst_feq0_mediumvariability.%s", fig.ext)
knitr::include_graphics(img)
```

```{r graphsceq0simulfst, echo=FALSE, out.width="90%", fig.cap="Moyenne des écarts relatifs des révisions entre la première et la deuxième estimation ($MAE_{ce}(0)$), comparativement aux révisions du filtre LC sur les séries à variabilité moyenne."}
img <- sprintf("img/simulations/fst_ceq0_mediumvariability.%s", fig.ext)
knitr::include_graphics(img)
```

En somme, même si une étude plus approfondie devrait être menée, pour l'étude de la méthode FST il parait opportun de se concentrer sur les filtres qui préserve les polynômes de degré 2 avec un poids associé au critère *fidelity* élevé et un poids faible associé au critère *smoothness*.






\newpage
